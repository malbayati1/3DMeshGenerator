{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 : Projection & Triangulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Please edit the cell below to include your name and student ID #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**name:**\n",
    "\n",
    "**SID:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cameras \n",
    "\n",
    "Please write out your answers in the empty cells below each question.  Feel free to include images, diagrams or equations as needed to explain your answer. For written answers, you can create nicely typeset equations in the notebook using MathJax/Latex (see \n",
    "https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Typesetting%20Equations.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 1.1 Field of View [5pts]\n",
    "Suppose your camera has an image sensor that is 1280 pixels wide and 1024 pixels tall with a physical resolution of 15pixels/mm and a focal length of f=50mm. **(a)** What is the horizontal field of view (in degrees)? **(b)** What is the vertical field of view? **(c)** Suppose you adjust the zoom on the camera, changing the focal length to 100mm. What is the new horizontal field of view? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***your answers go here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 1.2  Camera motions [5pts]\n",
    "\n",
    "Your camera starts out at the origin of the world coordinate system where as you stand holding the camera, the x-axis is pointed to the right, the y-axis is pointed down and z-axis is pointed out of the camera away from you.  You rotate the camera to the right about the y-axis by 90 degrees (clockwise looking down on the camera from above) and then translate it left along the x-axis by 2 meters.   **(a)** Describe this motion of the camera concisely by specifying the rotation matrix and translation vector corresponding to this motion.  **(b)** Suppose there is a point with coordinates (2,2,2) meters in the world coordinate system.  What will its coordinates be relative to the camera coordinate system after the camera has been moved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***your answers go here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 2. Projection\n",
    "\n",
    "### 2.1 Implement Projection [30pts]\n",
    "\n",
    "The code below outlines a simple python class that encapsulates the parameters of a camera. Write a function **project** which carries out the operation of projection with this camera. Your function should take as input the coordinates of a set of points in 3D, carry out projection based on the camera's parameters, and return the 2D coordinates of where those points would appear in the image. \n",
    "\n",
    "One approach is to combine all the camera parameters into a single 3x4 camera matrix as we discussed in class. However, for the purpose of the assignment it is sufficient to simply carry out each step separately (i.e., convert from global to camera coordinate system, project into camera, scale by focal length and offset by principal point). In either case, your code **should not** involve any for-loops over individual points.  Instead please carry out the computation using vectorized numpy operations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import visutils  #provided visutils.py contains some helper functions for 3d plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Camera:\n",
    "    \"\"\"\n",
    "    A simple data structure describing camera parameters \n",
    "    \n",
    "    The parameters describing the camera\n",
    "    cam.f : float   --- camera focal length (in units of pixels)\n",
    "    cam.c : 2x1 vector  --- offset of principle point\n",
    "    cam.R : 3x3 matrix --- camera rotation\n",
    "    cam.t : 3x1 vector --- camera translation \n",
    "\n",
    "    \n",
    "    \"\"\"    \n",
    "    def __init__(self,f,c,R,t):\n",
    "        self.f = f\n",
    "        self.c = c\n",
    "        self.R = R\n",
    "        self.t = t\n",
    "\n",
    "        \n",
    "    def project(self,pts3):\n",
    "        \"\"\"\n",
    "        Project the given 3D points in world coordinates into the specified camera    \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pts3 : 2D numpy.array (dtype=float)\n",
    "            Coordinates of N points stored in a array of shape (3,N)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pts2 : 2D numpy.array (dtype=float)\n",
    "            Image coordinates of N points stored in an array of shape (2,N)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        assert(pts3.shape[0]==3)\n",
    "\n",
    "        #\n",
    "        # your code goes here\n",
    "        #\n",
    "        \n",
    "        assert(pts2.shape[1]==pts3.shape[1])\n",
    "        assert(pts2.shape[0]==2)\n",
    "    \n",
    "        return pts2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Testing projection [10pts]\n",
    "\n",
    "To test your camera projection operation, we will create a synthetic scene, place the camera in the scene, and visualize the resulting \"image\" of the scene. The function provided below, ***generate_hemisphere*** creates a set of 3d points randomly distributed on the surface of a hemisphere. We use this to construct a simple test scene and visualize the result of calling your ***project*** function.\n",
    "\n",
    "For the scene geometry, generate 500 points on a hemisphere of radius 1 centered at [5,0,5]. For the camera, place the camera at the origin [0,0,0] and rotate the camera so that the camera is looking directly at the center of the hemisphere. Let's suppose our camera has a 100x100 pixel sensor. Select the focal length and principal point of the camera so that the image of the hemisphere points is 100 pixels tall and centered in the image (i.e., the projected points should all fall in the square [0-100]x[0-100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hemisphere(radius,center,npts):\n",
    "    \"\"\"\n",
    "    Generate a set of 3D points which are randomly distributed on the\n",
    "    surface of a hemisphere for purposes of testing your code.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    radius : float\n",
    "        Hemisphere radius\n",
    "\n",
    "    center : numpy.array (dtype=float)\n",
    "        3x1 vector specifying the center of the hemisphere\n",
    "        \n",
    "    npts : int\n",
    "        number of points to generate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : 2D numpy.array (dtype=float)\n",
    "        (3,npts) array containing coordinates of the points\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    assert(center.shape==(3,1))\n",
    "    \n",
    "    #generate randomly distributed points\n",
    "    x = np.random.standard_normal((3,npts))\n",
    "    \n",
    "    #scale points to the surface of a sphere with given radius\n",
    "    nx = np.sqrt(np.sum(x*x,axis=0))\n",
    "    x = radius * x / nx\n",
    "    \n",
    "    # make points with positive z-coordinates negative\n",
    "    # so that points are all on a half-sphere \n",
    "    x[2,:] = -np.abs(x[2,:])\n",
    "    \n",
    "    # translate to desired position\n",
    "    x = x + center\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# test your camera and project function\n",
    "#\n",
    "\n",
    "# generate 500 3D points on a hemisphere of radius 1 at\n",
    "# a location 5 units along the z axis and 5 units along\n",
    "# the y axis.\n",
    "pts3 = generate_hemisphere(...)\n",
    "\n",
    "# create the camera with the desired parameters\n",
    "cam = Camera(...)\n",
    "\n",
    "# call the project function to see where the points\n",
    "# appear in the camera image\n",
    "pts2 = cam.project(pts3)\n",
    "\n",
    "\n",
    "#\n",
    "# Visualize results.  You do not need to change the code below, but\n",
    "# make sure you understand it so you can use it in the future to \n",
    "# perform your own visualizations.\n",
    "#\n",
    "\n",
    "# generate coordinates of a line segment running from the center\n",
    "# of the camera to 2 units in front of the camera along the z-axis\n",
    "# this is useful for visualizing what direction the camera is pointed\n",
    "# at in the plots below\n",
    "look = np.hstack((cam.t,cam.t+cam.R @ np.array([[0,0,2]]).T))\n",
    "\n",
    "# visualize the image of the points in the camera\n",
    "# draw a square [0,100]x[0,100] designating the box\n",
    "# we want the hemisphere to fall inside of\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(2,2,1)\n",
    "ax.plot(pts2[0,:],pts2[1,:],'.')\n",
    "ax.add_patch(patches.Rectangle((0,0),100,100,color='r',fill=False))\n",
    "plt.grid()\n",
    "plt.axis('square')\n",
    "plt.title('camera projection')\n",
    "\n",
    "# overhead view (xz-plane) showing points, camera\n",
    "# position, and direction camera is pointed\n",
    "ax = fig.add_subplot(2,2,2)\n",
    "ax.plot(pts3[0,:],pts3[2,:],'.')\n",
    "ax.plot(cam.t[0],cam.t[2],'ro')\n",
    "ax.plot(look[0,:],look[2,:],'r')\n",
    "plt.axis('equal')\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('z')\n",
    "plt.title('scene overhead view')\n",
    "\n",
    "# side view (yz-plane) showing points, camera\n",
    "# position, and direction camera is pointed\n",
    "ax = fig.add_subplot(2,2,3)\n",
    "ax.plot(pts3[2,:],pts3[1,:],'.')\n",
    "ax.plot(cam.t[2],cam.t[1],'ro')\n",
    "ax.plot(look[2,:],look[1,:],'r')\n",
    "plt.axis('equal')\n",
    "plt.grid()\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('y')\n",
    "plt.title('scene side view')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Triangulation \n",
    "\n",
    "## 3.1 Implementing Triangulation [40pts]\n",
    "\n",
    "Write a function called **triangulate** that takes the coordinates of points in two images along with the camera parameters and returns the 3D coordinates of the points in world coordinates.  Please use the least-squares technique we described in class.  You can use **np.linalg.lstsq** to get the least-squares estimate of the z coordinates for each point. Since the linear system is independent for each point, it is fine to use a for-loop over the points to be triangulated. \n",
    "\n",
    "The solution we described in the Lecture 4 slides assumes the right camera is at the world coordinate origin.  To make this work for cameras in general position (neither camera at the origin), you will first need to determine the relative pose between the two cameras, solve the least squares problem to get the z-coordinates and then transform the solution back to the world coordinate system. Please refer to class notes and slides.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangulate(pts2L,camL,pts2R,camR):\n",
    "    \"\"\"\n",
    "    Triangulate the set of points seen at location pts2L / pts2R in the\n",
    "    corresponding pair of cameras. Return the 3D coordinates relative\n",
    "    to the global coordinate system\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pts2L : 2D numpy.array (dtype=float)\n",
    "        Coordinates of N points stored in a array of shape (2,N) seen from camL camera\n",
    "\n",
    "    pts2R : 2D numpy.array (dtype=float)\n",
    "        Coordinates of N points stored in a array of shape (2,N) seen from camR camera\n",
    "\n",
    "    camL : Camera\n",
    "        The first \"left\" camera view\n",
    "\n",
    "    camR : Camera\n",
    "        The second \"right\" camera view\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pts3 : 2D numpy.array (dtype=float)\n",
    "        (3,N) array containing 3D coordinates of the points in global coordinates\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #\n",
    "    # Your code goes here.  I recommend adding assert statements to check the\n",
    "    # sizes of the inputs and outputs to make sure they are correct \n",
    "    #\n",
    "\n",
    "\n",
    "    return pts3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Testing Triangulation [10pts]\n",
    "\n",
    "The provided code below creates a simple test case of two cameras offset to the right and left of the origin and rotated to point at a hemisphere placed some distance from the origin along the z-axis.  It calls your **project** function to generate the left and right image and then calls your **triangulate** function to recover the 3D point locations. This corresponds to the \"noise free\" case where our camera calibration and point localization are perfect by construction.\n",
    "\n",
    "You should **(a)** understand and run the code below to verify that when you triangulate the two sets of points you get back the original 3D locations.  **(b)** Write an additional block of code which triangulates the exact same set of 2D points (pts2L,pts2R) but calls your triangulation code passing it a poorly calibrated camera. You can simulate this by adjusting the focal length of camL to be 10% larger. Visualize the 3D reconstruction of the perfectly calibrated cameras and the poorly calibrated cameras. **(c)** to quantify the error in the reconstruction, compute the average over all points of the distance between the true 3D location and the reconstruction 3D location. Report this average error for the perfect case (it should be close to 0) and for the case where the focal length is off by 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# create a test scene with two cameras \n",
    "#\n",
    "\n",
    "# utility function to create a rotation matrix representing rotation \n",
    "# around y-axis by amount theta\n",
    "def roty(theta):\n",
    "    st = np.sin(theta)\n",
    "    ct = np.cos(theta)\n",
    "    R = np.array([[ct,0,st],[0,1,0],[-st,0,ct]])\n",
    "    return R\n",
    "\n",
    "#compute rotation angle so that the camera is centered on the sphere\n",
    "b = 5  #baseline between cameras\n",
    "d = 10   #distance to object\n",
    "theta = np.arctan((b/2)/d)  #compute the rotation angle needed to point cameras at the hemisphere\n",
    "tL = np.array([[-(b/2),0,0]]).T\n",
    "tR = np.array([[(b/2),0,0]]).T\n",
    "camL = Camera(f=100,c=np.array([[50,50]]).T,t=tL,R=roty(theta))\n",
    "camR = Camera(f=100,c=np.array([[50,50]]).T,t=tR,R=roty(-theta))\n",
    "\n",
    "#generate 3D points\n",
    "pts3 = generate_hemisphere(2,np.array([[0,0,d]]).T,500)\n",
    "\n",
    "#project into each camera\n",
    "pts2L = camL.project(pts3)\n",
    "pts2R = camR.project(pts3)\n",
    "\n",
    "#triangulate to recover 3d position\n",
    "pts3t = triangulate(pts2L,camL,pts2R,camR)\n",
    "\n",
    "#\n",
    "# visualize results\n",
    "#\n",
    "\n",
    "# generate coordinates of a line segment running from the center\n",
    "# of the camera to 2 units in front of the camera\n",
    "lookL = np.hstack((tL,tL+camL.R @ np.array([[0,0,2]]).T))\n",
    "lookR = np.hstack((tR,tR+camR.R @ np.array([[0,0,2]]).T))\n",
    "\n",
    "# visualize the left and right image overlaid\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(2,2,1)\n",
    "ax.plot(pts2L[0,:],pts2L[1,:],'b.')\n",
    "ax.plot(pts2R[0,:],pts2R[1,:],'r.')\n",
    "plt.axis('equal')\n",
    "plt.legend(('camL','camR'),loc=1)\n",
    "\n",
    "#visualize 3D layout of points, camera positions\n",
    "# and the direction the camera is pointing\n",
    "ax = fig.add_subplot(2,2,2,projection='3d')\n",
    "ax.plot(pts3[0,:],pts3[1,:],pts3[2,:],'.')\n",
    "ax.plot(tR[0],tR[1],tR[2],'ro')\n",
    "ax.plot(tL[0],tL[1],tL[2],'bo')\n",
    "ax.plot(lookL[0,:],lookL[1,:],lookL[2,:],'b')\n",
    "ax.plot(lookR[0,:],lookR[1,:],lookR[2,:],'r')\n",
    "visutils.set_axes_equal_3d(ax)\n",
    "visutils.label_axes(ax)\n",
    "plt.title('scene 3D view')\n",
    "\n",
    "# overhead view showing points, camera\n",
    "# positions, and direction camera is pointed\n",
    "ax = fig.add_subplot(2,2,3)\n",
    "ax.plot(pts3[0,:],pts3[2,:],'.')\n",
    "ax.plot(tL[0],tL[2],'bo')\n",
    "ax.plot(lookL[0,:],lookL[2,:],'b')\n",
    "ax.plot(tR[0],tR[2],'ro')\n",
    "ax.plot(lookR[0,:],lookR[2,:],'r')\n",
    "plt.axis('equal')\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('z')\n",
    "plt.title('scene overhead view')\n",
    "\n",
    "# compare reconstruction\n",
    "ax = fig.add_subplot(2,2,4,projection='3d')\n",
    "ax.plot(pts3[0,:],pts3[1,:],pts3[2,:],'b.')\n",
    "ax.plot(pts3t[0,:],pts3t[1,:],pts3t[2,:],'ro',fillstyle='none')\n",
    "visutils.set_axes_equal_3d(ax)\n",
    "visutils.label_axes(ax)\n",
    "plt.title('reconstruction overlay')\n",
    "plt.legend(('true','reconstruction'),loc=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# triangulate the points again but modify camL so that the focal length\n",
    "# is 10% larger than the true value used to produce the 2d points\n",
    "#\n",
    "\n",
    "\n",
    "pts3t_badcalib = ...\n",
    "\n",
    "\n",
    "#\n",
    "# visualize reconstruction compared to noise-free reconstruction.  \n",
    "# how do they differ from each other??\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# compute the average reconstruction error (distance between the true 3D location and\n",
    "# triangulated location, averaged over all the points) for both the perfect case (pts3t)\n",
    "# and for the badly calibrated case (pts3t_badcalib) and print out the resulting errors\n",
    "# in the notebook\n",
    "#\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
